{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NB2_Autograd.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Dr70wttBGNnJ"},"source":["## Autograd\n","Autograd is now a core torch package for automatic differentiation. "]},{"cell_type":"code","metadata":{"id":"-6hFwBUOGKU9"},"source":["import torch"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cIJC4GgjIdVs","executionInfo":{"status":"ok","timestamp":1630585121483,"user_tz":240,"elapsed":223,"user":{"displayName":"Maohua Liu","photoUrl":"","userId":"17541663534354863927"}},"outputId":"8cf549a3-c43b-421e-acb9-6068096f978b"},"source":["x = torch.tensor([[1., 1.], [2., 2.]], requires_grad=True)\n","print(x)\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 1.],\n","        [2., 2.]], requires_grad=True)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vv5Zi0tnI8Vj","executionInfo":{"status":"ok","timestamp":1630585124886,"user_tz":240,"elapsed":104,"user":{"displayName":"Maohua Liu","photoUrl":"","userId":"17541663534354863927"}},"outputId":"cce94b24-fc81-4d69-ba22-2d9180129bfb"},"source":["y = x*x\n","print(y)\n","\n","z = y.sum()\n","\n","print(z)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 1.],\n","        [4., 4.]], grad_fn=<MulBackward0>)\n","tensor(10., grad_fn=<SumBackward0>)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7kDd1nioJnmV","executionInfo":{"status":"ok","timestamp":1630585155363,"user_tz":240,"elapsed":227,"user":{"displayName":"Maohua Liu","photoUrl":"","userId":"17541663534354863927"}},"outputId":"d5702167-fe89-42c5-e8bb-1689a5ba7f59"},"source":["\n","z.backward()\n","print(x.grad)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[2., 2.],\n","        [4., 4.]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"FoAdE6scPN3x"},"source":["## Let us see another example"]},{"cell_type":"code","metadata":{"id":"7MQ3tINZKI7Z"},"source":["a = torch.tensor([1., 2.], requires_grad=True)\n","b = torch.tensor([3., 4.], requires_grad=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J5ZSYQz3PbtX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630585231056,"user_tz":240,"elapsed":97,"user":{"displayName":"Maohua Liu","photoUrl":"","userId":"17541663534354863927"}},"outputId":"74caba09-c299-4d1d-a306-96311c270a66"},"source":["Q = 2*a**3 - b**2\n","print(Q)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([-7.,  0.], grad_fn=<SubBackward0>)\n"]}]},{"cell_type":"markdown","metadata":{"id":"rEwH8VEZP783"},"source":["If output is a scalar, we can just call $.backward$.  Since Q is a vector, we need to pass gradient argument, which  is a tensor of the same shape as Q. It represents the gradient of Q w.r.t. itself."]},{"cell_type":"code","metadata":{"id":"SDQ-yrhzP44p"},"source":["e_grad = torch.tensor([1., 1.])\n","Q.backward(gradient=e_grad)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gMER8cSsROQn","executionInfo":{"status":"ok","timestamp":1630028520293,"user_tz":240,"elapsed":264,"user":{"displayName":"James Yang","photoUrl":"","userId":"05522971510241172115"}},"outputId":"deee9955-952b-4441-8187-ffb544cb1e88"},"source":["print(a.grad, b.grad)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([ 6., 24.]) tensor([-6., -8.])\n"],"name":"stdout"}]}]}