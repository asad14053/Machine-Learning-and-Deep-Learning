# -*- coding: utf-8 -*-
"""Assignment_1: Md Asaduzzaman (811562873)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oEQFstNhZ362Jvu2jpImSV-KeaqNbpw3

## Assignment 1 - Gradient Descent Algorithms with PyTorch
The following code cell is used to generate some noisy data (don't change this code).  You will need to use the data generated here to train a linear model $y=ax+b$ (i.e., Perform gradient descent on the two parameters $a$ and $b$). 


1.   You will need to code two algorithms (1) Gradient Descent + regular momentum, and (2) Adam, using PyTorch "autograd".
2.   Check your results against the linear regression model. You can do linear regression using "sklearn.linear_model" (Hint: you need to convert tensor to numpy array.)
"""

import torch
torch.manual_seed(42)
x = torch.rand(100,1)
y = 1 + 2 * x + .1 * torch.rand(100, 1)

a = torch.randn(1, requires_grad=True)
b = torch.randn(1, requires_grad=True)
a, b

"""## Your code goes here:

# Model and loading pytorch package
"""

import time
start_time = time.time()
# print('start_time: ', start_time)
!pip install PySimpleGUI
import torch
import PySimpleGUI as sg

!pip install matplotlib
import matplotlib
matplotlib.use('tkagg')
from matplotlib.backends.backend_tkagg import FigureCanvasAgg
import matplotlib.backends.backend_tkagg as tkagg
import tkinter as Tk


import numpy as np
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt
import math
import pandas as pd
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
import sys
import pickle

from torch import nn, optim
from torchvision import transforms
from collections import OrderedDict

############     Linear regression model - 3 hidden layers    #####################
epochs = 500
input_dim = 1
output_dim = 1
dropout_prob = 0.1
nodes_hidden_1 = 50
nodes_hidden_2 = 50
nodes_hidden_3 = 50
## nn.Linear() is fully connected layer  
model = nn.Sequential(OrderedDict([
                        ('fc1', nn.Linear(input_dim, nodes_hidden_1)),   
                        ('relu', nn.ReLU()),     #ReLu nonlinear activation
                        ('dropout', nn.Dropout(dropout_prob)),
                        ('fc2', nn.Linear(nodes_hidden_1, nodes_hidden_2)),
                        ('relu', nn.ReLU()),      #ReLu nonlinear activation
                        ('dropout', nn.Dropout(dropout_prob)),
                        ('fc3', nn.Linear(nodes_hidden_2, nodes_hidden_3)),
                        ('relu', nn.ReLU()),       #ReLu nonlinear activation
                        ('dropout', nn.Dropout(dropout_prob)),
                        ('fc4', nn.Linear(nodes_hidden_3, output_dim)),
                        ]))

"""# [1] (a)Gradient Descent + regular momentum"""

y_predict = a*x+b

inputs = x;
labels =y;

# learning rate and momentum for SDG
learning_rate = 0.0001
momentum = 0.9

# MSE loss algorithm
criterion = nn.MSELoss()  

# Gradient Descent + regular momentum optimizer
optimizer = optim.SGD(model.parameters(), learning_rate, momentum)  

running_loss = []
running_loss_validation = []

#number of iteration for learning
epochs = 500
for epoch in range(epochs):
  epoch+=1

  #################   train the model   ######################
  model.train()    # prep model for training

  # Clear gradients w.r.t. parameters, else gradients will be added up with every previous pass
  optimizer.zero_grad() 

  # Forward to get output
  outputs = model(inputs)

  # Calculate Loss
  loss = criterion(outputs, labels)       ## mean squared error
  
  # Getting gradients w.r.t. parameters
  loss.backward()
    
  # Updating parameters (weight and bias)
  optimizer.step()         ## take a step with optimizer to update the weights
  running_loss.append(loss.item())

  print('epoch: {}, mse_loss: {:.6f}'.format(epoch, loss.item()))

"""# [1] (b)Adam, using PyTorch "autograd"


"""

y_predict = a*x+b

inputs = x;
labels =y;

# learning rate for Adam
learning_rate = 0.0001

# MSE loss algorithm
criterion = nn.MSELoss()  

# Adam, using PyTorch "autograd" optimizer
optimizer = optimizer = optim.Adam(model.parameters(), learning_rate)

running_loss = []
running_loss_validation = []

#number of iteration for learning
epochs = 500
for epoch in range(epochs):
  epoch+=1

  #################   train the model   ######################
  model.train()    # prep model for training

  # Clear gradients w.r.t. parameters, else gradients will be added up with every previous pass
  optimizer.zero_grad() 

  # Forward to get output
  outputs = model(inputs)

  # Calculate Loss
  loss = criterion(outputs, labels)       ## mean squared error
  
  # Getting gradients w.r.t. parameters
  loss.backward()
    
  # Updating parameters (weight and bias)
  optimizer.step()         ## take a step with optimizer to update the weights
  running_loss.append(loss.item())

  print('epoch: {}, mse_loss: {:.6f}'.format(epoch, loss.item()))

"""# [2] Check your results against the linear regression model"""

# data for regression fitting
x= x.numpy()
y= y.numpy()
print ([x])
print([y])

# regressor fitting
reg = LinearRegression().fit(x, y)
print(reg.score(x, y))
print(reg.coef_)
print(reg.intercept_)
print(f'Result: y = {reg.intercept_[0]} + {reg.coef_[0][0]} x ')
y_fit = reg.coef_ * x + reg.intercept_;

# plot the loss and regression fitting

mse_training_interval = 10
mse_validation_interval = 10
running_loss = running_loss[::mse_training_interval]
running_loss_index = [i for i in range(1, epochs, mse_training_interval)]
print ([running_loss])
print([running_loss_index])

## Plot the regression model
plt.figure()
plt.suptitle('(epochs-{}) - pyTorch'.format(epochs), fontsize=25, fontweight='bold')

plt.subplot(211)
plt.plot(running_loss_index, running_loss, 'r-', linewidth=3, label='mse_loss_train')
plt.xlabel('epochs', fontsize=15)
plt.ylabel('MSE Loss', fontsize=15)

plt.subplot(212)
plt.plot(x, y, 'r-', linewidth=3, label='mse_loss_train')
plt.plot(x, y_fit, 'b-', linewidth=3, label='mse_loss_train')
plt.xlabel('x', fontsize=15)
plt.ylabel('Fitting', fontsize=15)

plt.show()