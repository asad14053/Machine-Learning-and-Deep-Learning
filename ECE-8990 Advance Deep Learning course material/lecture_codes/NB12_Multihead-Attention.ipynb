{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"colab":{"name":"NB12_Multihead-Attention.ipynb","provenance":[{"file_id":"https://github.com/d2l-ai/d2l-pytorch-colab/blob/master/chapter_attention-mechanisms/multihead-attention.ipynb","timestamp":1634416165762}],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"_cskkEHxy0uB"},"source":["!pip install d2l==0.17.0\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"origin_pos":0,"id":"pPn4OKHJy0uD"},"source":["# Multi-Head Attention\n","![Multi-head attention, where multiple heads are concatenated then linearly transformed.](https://github.com/d2l-ai/d2l-pytorch-colab/blob/master/img/multi-head-attention.svg?raw=1)\n","bold text\n","\n","\n","### Model\n","\n","Before providing the implementation of multi-head attention,\n","let us formalize this model mathematically.\n","Given a query $\\mathbf{q} \\in \\mathbb{R}^{d_q}$,\n","a key $\\mathbf{k} \\in \\mathbb{R}^{d_k}$,\n","and a value $\\mathbf{v} \\in \\mathbb{R}^{d_v}$,\n","each attention head $\\mathbf{h}_i$  ($i = 1, \\ldots, h$)\n","is computed as\n","\n","$$\\mathbf{h}_i = f(\\mathbf W_i^{(q)}\\mathbf q, \\mathbf W_i^{(k)}\\mathbf k,\\mathbf W_i^{(v)}\\mathbf v) \\in \\mathbb R^{p_v},$$\n","\n","where learnable parameters\n","$\\mathbf W_i^{(q)}\\in\\mathbb R^{p_q\\times d_q}$,\n","$\\mathbf W_i^{(k)}\\in\\mathbb R^{p_k\\times d_k}$\n","and $\\mathbf W_i^{(v)}\\in\\mathbb R^{p_v\\times d_v}$,\n","and\n","$f$ is attention pooling,\n","such as\n","additive attention and scaled dot-product attention.\n","The multi-head attention output\n","is another linear transformation via \n","learnable parameters\n","$\\mathbf W_o\\in\\mathbb R^{p_o\\times h p_v}$\n","of the concatenation of $h$ heads:\n","\n","$$\\mathbf W_o \\begin{bmatrix}\\mathbf h_1\\\\\\vdots\\\\\\mathbf h_h\\end{bmatrix} \\in \\mathbb{R}^{p_o}.$$\n","\n","Based on this design,\n","each head may attend to different parts of the input.\n","More sophisticated functions than the simple weighted average\n","can be expressed.\n"]},{"cell_type":"code","metadata":{"origin_pos":2,"tab":["pytorch"],"id":"PZgCzESIy0uF"},"source":["import math\n","import torch\n","from torch import nn\n","from d2l import torch as d2l"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"origin_pos":4,"id":"KKSR3bB-y0uG"},"source":["### Implementation\n","\n","In our implementation,\n","we [**choose the scaled dot-product attention\n","for each head**] of the multi-head attention.\n","To avoid significant growth\n","of computational cost and parameterization cost,\n","we set\n","$p_q = p_k = p_v = p_o / h$.\n","Note that $h$ heads\n","can be computed in parallel\n","if we set\n","the number of outputs of linear transformations\n","for the query, key, and value\n","to $p_q h = p_k h = p_v h = p_o$.\n","In the following implementation,\n","$p_o$ is specified via the argument `num_hiddens`.\n"]},{"cell_type":"code","metadata":{"origin_pos":6,"tab":["pytorch"],"id":"U7sOl6ECy0uH"},"source":["class MultiHeadAttention(nn.Module):\n","    \"\"\"Multi-head attention.\"\"\"\n","    def __init__(self, key_size, query_size, value_size, num_hiddens,\n","                 num_heads, dropout, bias=False, **kwargs):\n","        super(MultiHeadAttention, self).__init__(**kwargs)\n","        self.num_heads = num_heads\n","        self.attention = d2l.DotProductAttention(dropout)\n","        self.W_q = nn.Linear(query_size, num_hiddens, bias=bias)\n","        self.W_k = nn.Linear(key_size, num_hiddens, bias=bias)\n","        self.W_v = nn.Linear(value_size, num_hiddens, bias=bias)\n","        self.W_o = nn.Linear(num_hiddens, num_hiddens, bias=bias)\n","\n","    def forward(self, queries, keys, values, valid_lens):\n","        # Shape of `queries`, `keys`, or `values`:\n","        # (`batch_size`, no. of queries or key-value pairs, `num_hiddens`)\n","        # Shape of `valid_lens`:\n","        # (`batch_size`,) or (`batch_size`, no. of queries)\n","        # After transposing, shape of output `queries`, `keys`, or `values`:\n","        # (`batch_size` * `num_heads`, no. of queries or key-value pairs,\n","        # `num_hiddens` / `num_heads`)\n","        queries = transpose_qkv(self.W_q(queries), self.num_heads)\n","        keys = transpose_qkv(self.W_k(keys), self.num_heads)\n","        values = transpose_qkv(self.W_v(values), self.num_heads)\n","\n","        if valid_lens is not None:\n","            # On axis 0, copy the first item (scalar or vector) for\n","            # `num_heads` times, then copy the next item, and so on\n","            valid_lens = torch.repeat_interleave(valid_lens,\n","                                                 repeats=self.num_heads,\n","                                                 dim=0)\n","\n","        # Shape of `output`: (`batch_size` * `num_heads`, no. of queries,\n","        # `num_hiddens` / `num_heads`)\n","        output = self.attention(queries, keys, values, valid_lens)\n","\n","        # Shape of `output_concat`:\n","        # (`batch_size`, no. of queries, `num_hiddens`)\n","        output_concat = transpose_output(output, self.num_heads)\n","        return self.W_o(output_concat)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"origin_pos":8,"id":"sKf9dOgQy0uI"},"source":["To allow for [**parallel computation of multiple heads**],\n","the above `MultiHeadAttention` class uses two transposition functions as defined below.\n","Specifically,\n","the `transpose_output` function reverses the operation\n","of the `transpose_qkv` function.\n"]},{"cell_type":"code","metadata":{"origin_pos":10,"tab":["pytorch"],"id":"zF7UHz3ey0uJ"},"source":["def transpose_qkv(X, num_heads):\n","    \"\"\"Transposition for parallel computation of multiple attention heads.\"\"\"\n","    # Shape of input `X`:\n","    # (`batch_size`, no. of queries or key-value pairs, `num_hiddens`).\n","    # Shape of output `X`:\n","    # (`batch_size`, no. of queries or key-value pairs, `num_heads`,\n","    # `num_hiddens` / `num_heads`)\n","    X = X.reshape(X.shape[0], X.shape[1], num_heads, -1)\n","\n","    # Shape of output `X`:\n","    # (`batch_size`, `num_heads`, no. of queries or key-value pairs,\n","    # `num_hiddens` / `num_heads`)\n","    X = X.permute(0, 2, 1, 3)\n","\n","    # Shape of `output`:\n","    # (`batch_size` * `num_heads`, no. of queries or key-value pairs,\n","    # `num_hiddens` / `num_heads`)\n","    return X.reshape(-1, X.shape[2], X.shape[3])\n","\n","def transpose_output(X, num_heads):\n","    \"\"\"Reverse the operation of `transpose_qkv`.\"\"\"\n","    X = X.reshape(-1, num_heads, X.shape[1], X.shape[2])\n","    X = X.permute(0, 2, 1, 3)\n","    return X.reshape(X.shape[0], X.shape[1], -1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"origin_pos":12,"id":"W8NfEcquy0uJ"},"source":["Let us [**test our implemented**] `MultiHeadAttention` class\n","using a toy example where keys and values are the same.\n","As a result,\n","the shape of the multi-head attention output\n","is (`batch_size`, `num_queries`, `num_hiddens`).\n"]},{"cell_type":"code","metadata":{"origin_pos":14,"tab":["pytorch"],"id":"KIrLd9Hly0uK","outputId":"f191ff84-e97a-424c-c34c-d857c3df24bb"},"source":["num_hiddens, num_heads = 100, 5\n","attention = MultiHeadAttention(num_hiddens, num_hiddens, num_hiddens,\n","                               num_hiddens, num_heads, 0.5)\n","attention.eval()"],"execution_count":null,"outputs":[{"data":{"text/plain":["MultiHeadAttention(\n","  (attention): DotProductAttention(\n","    (dropout): Dropout(p=0.5, inplace=False)\n","  )\n","  (W_q): Linear(in_features=100, out_features=100, bias=False)\n","  (W_k): Linear(in_features=100, out_features=100, bias=False)\n","  (W_v): Linear(in_features=100, out_features=100, bias=False)\n","  (W_o): Linear(in_features=100, out_features=100, bias=False)\n",")"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"origin_pos":16,"tab":["pytorch"],"id":"ZCYTuG2Wy0uM","outputId":"f58daaa2-2884-495c-efe0-c5d53947845a"},"source":["batch_size, num_queries, num_kvpairs, valid_lens = 2, 4, 6, torch.tensor([\n","    3, 2])\n","X = torch.ones((batch_size, num_queries, num_hiddens))\n","Y = torch.ones((batch_size, num_kvpairs, num_hiddens))\n","attention(X, Y, Y, valid_lens).shape"],"execution_count":null,"outputs":[{"data":{"text/plain":["torch.Size([2, 4, 100])"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","metadata":{"origin_pos":18,"id":"EG9QVozMy0uM"},"source":["### MHA Summary\n","\n","* Multi-head attention combines knowledge of the same attention pooling via different representation subspaces of queries, keys, and values.\n","* To compute multiple heads of multi-head attention in parallel, proper tensor manipulation is needed.\n"]},{"cell_type":"markdown","metadata":{"id":"wJZz1fbJ0E5u"},"source":["#Self-Attention\n","\n","Given a sequence of input tokens\n","$\\mathbf{x}_1, \\ldots, \\mathbf{x}_n$ where any $\\mathbf{x}_i \\in \\mathbb{R}^d$ ($1 \\leq i \\leq n$),\n","its self-attention outputs\n","a sequence of the same length\n","$\\mathbf{y}_1, \\ldots, \\mathbf{y}_n$,\n","where\n","\n","$$\\mathbf{y}_i = f(\\mathbf{x}_i, (\\mathbf{x}_1, \\mathbf{x}_1), \\ldots, (\\mathbf{x}_n, \\mathbf{x}_n)) \\in \\mathbb{R}^d$$\n","\n","according to the definition of attention pooling $f$ in\n",":eqref:`eq_attn-pooling`.\n","Using multi-head attention,\n","the following code snippet\n","computes the self-attention of a tensor\n","with shape (batch size, number of time steps or sequence length in tokens, $d$).\n","The output tensor has the same shape.\n"]},{"cell_type":"code","metadata":{"id":"3GR4cuHp0JT4"},"source":["num_hiddens, num_heads = 100, 5\n","attention = d2l.MultiHeadAttention(num_hiddens, num_hiddens, num_hiddens,\n","                                   num_hiddens, num_heads, 0.5)\n","attention.eval()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pN2epeD50jGV"},"source":["batch_size, num_queries, valid_lens = 2, 4, torch.tensor([3, 2])\n","X = torch.ones((batch_size, num_queries, num_hiddens))\n","attention(X, X, X, valid_lens).shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8QhMtlLh0o0h"},"source":["#Positional Encoding\n","Unlike RNNs that recurrently process\n","tokens of a sequence one by one,\n","self-attention ditches\n","sequential operations in favor of \n","parallel computation.\n","To use the sequence order information,\n","we can inject\n","absolute or relative\n","positional information\n","by adding *positional encoding*\n","to the input representations.\n","Positional encodings can be \n","either learned or fixed.\n","In the following, \n","we describe a fixed positional encoding\n","based on sine and cosine functions :cite:`Vaswani.Shazeer.Parmar.ea.2017`.\n","\n","Suppose that\n","the input representation $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ contains the $d$-dimensional embeddings for $n$ tokens of a sequence.\n","The positional encoding outputs\n","$\\mathbf{X} + \\mathbf{P}$\n","using a positional embedding matrix $\\mathbf{P} \\in \\mathbb{R}^{n \\times d}$ of the same shape,\n","whose element on the $i^\\mathrm{th}$ row \n","and the $(2j)^\\mathrm{th}$\n","or the $(2j + 1)^\\mathrm{th}$ column is\n","\n","$$\\begin{aligned} p_{i, 2j} &= \\sin\\left(\\frac{i}{10000^{2j/d}}\\right),\\\\p_{i, 2j+1} &= \\cos\\left(\\frac{i}{10000^{2j/d}}\\right).\\end{aligned}$$\n","\n","At first glance,\n","this trigonometric-function\n","design looks weird.\n","Before explanations of this design,\n","let us first implement it in the following `PositionalEncoding` class."]},{"cell_type":"code","metadata":{"id":"8ARXjZ_B0pcl"},"source":["class PositionalEncoding(nn.Module):\n","    \"\"\"Positional encoding.\"\"\"\n","    def __init__(self, num_hiddens, dropout, max_len=1000):\n","        super(PositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(dropout)\n","        # Create a long enough `P`\n","        self.P = torch.zeros((1, max_len, num_hiddens))\n","        X = torch.arange(max_len, dtype=torch.float32).reshape(\n","            -1, 1) / torch.pow(\n","                10000,\n","                torch.arange(0, num_hiddens, 2, dtype=torch.float32) /\n","                num_hiddens)\n","        self.P[:, :, 0::2] = torch.sin(X)\n","        self.P[:, :, 1::2] = torch.cos(X)\n","\n","    def forward(self, X):\n","        X = X + self.P[:, :X.shape[1], :].to(X.device)\n","        return self.dropout(X)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LYWUkVKo03eW"},"source":["In the positional embedding matrix $\\mathbf{P}$,\n","[**rows correspond to positions within a sequence\n","and columns represent different positional encoding dimensions**].\n","In the example below,\n","we can see that\n","the $6^{\\mathrm{th}}$ and the $7^{\\mathrm{th}}$\n","columns of the positional embedding matrix \n","have a higher frequency than \n","the $8^{\\mathrm{th}}$ and the $9^{\\mathrm{th}}$\n","columns.\n","The offset between \n","the $6^{\\mathrm{th}}$ and the $7^{\\mathrm{th}}$ (same for the $8^{\\mathrm{th}}$ and the $9^{\\mathrm{th}}$) columns\n","is due to the alternation of sine and cosine functions."]},{"cell_type":"code","metadata":{"id":"Oj-dNJuG04Fk"},"source":["encoding_dim, num_steps = 32, 60\n","pos_encoding = PositionalEncoding(encoding_dim, 0)\n","pos_encoding.eval()\n","X = pos_encoding(torch.zeros((1, num_steps, encoding_dim)))\n","P = pos_encoding.P[:, :X.shape[1], :]\n","d2l.plot(torch.arange(num_steps), P[0, :, 6:10].T, xlabel='Row (position)',\n","         figsize=(6, 2.5), legend=[\"Col %d\" % d for d in torch.arange(6, 10)])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TbW6VKMv1Kf0"},"source":["### Absolute Positional Information\n","\n","To see how the monotonically decreased frequency\n","along the encoding dimension relates to absolute positional information,\n","let us print out [**the binary representations**] of $0, 1, \\ldots, 7$.\n","As we can see,\n","the lowest bit, the second-lowest bit, and the third-lowest bit alternate on every number, every two numbers, and every four numbers, respectively."]},{"cell_type":"code","metadata":{"id":"I8PcQDD81NHx"},"source":["for i in range(8):\n","    print(f'{i} in binary is {i:>03b}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZqUn1p7F1SwY"},"source":["In binary representations,\n","a higher bit has a lower frequency than a lower bit.\n","Similarly,\n","as demonstrated in the heat map below,\n","[**the positional encoding decreases\n","frequencies along the encoding dimension**]\n","by using trigonometric functions.\n","Since the outputs are float numbers,\n","such continuous representations\n","are more space-efficient\n","than binary representations.\n"]},{"cell_type":"code","metadata":{"id":"ZWPxYIIZ1V92"},"source":["P = P[0, :, :].unsqueeze(0).unsqueeze(0)\n","d2l.show_heatmaps(P, xlabel='Column (encoding dimension)',\n","                  ylabel='Row (position)', figsize=(3.5, 4), cmap='Blues')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Utni45VN1bJs"},"source":["### Relative Positional Information\n","\n","Besides capturing absolute positional information,\n","the above positional encoding\n","also allows\n","a model to easily learn to attend by relative positions.\n","This is because\n","for any fixed position offset $\\delta$,\n","the positional encoding at position $i + \\delta$\n","can be represented by a linear projection\n","of that at position $i$.\n","\n","\n","This projection can be explained\n","mathematically.\n","Denoting\n","$\\omega_j = 1/10000^{2j/d}$,\n","any pair of $(p_{i, 2j}, p_{i, 2j+1})$ \n","in :eqref:`eq_positional-encoding-def`\n","can \n","be linearly projected to $(p_{i+\\delta, 2j}, p_{i+\\delta, 2j+1})$\n","for any fixed offset $\\delta$:\n","\n","$$\\begin{aligned}\n","&\\begin{bmatrix} \\cos(\\delta \\omega_j) & \\sin(\\delta \\omega_j) \\\\  -\\sin(\\delta \\omega_j) & \\cos(\\delta \\omega_j) \\\\ \\end{bmatrix}\n","\\begin{bmatrix} p_{i, 2j} \\\\  p_{i, 2j+1} \\\\ \\end{bmatrix}\\\\\n","=&\\begin{bmatrix} \\cos(\\delta \\omega_j) \\sin(i \\omega_j) + \\sin(\\delta \\omega_j) \\cos(i \\omega_j) \\\\  -\\sin(\\delta \\omega_j) \\sin(i \\omega_j) + \\cos(\\delta \\omega_j) \\cos(i \\omega_j) \\\\ \\end{bmatrix}\\\\\n","=&\\begin{bmatrix} \\sin\\left((i+\\delta) \\omega_j\\right) \\\\  \\cos\\left((i+\\delta) \\omega_j\\right) \\\\ \\end{bmatrix}\\\\\n","=& \n","\\begin{bmatrix} p_{i+\\delta, 2j} \\\\  p_{i+\\delta, 2j+1} \\\\ \\end{bmatrix},\n","\\end{aligned}$$\n","\n","where the $2\\times 2$ projection matrix does not depend on any position index $i$.\n","\n"]}]}